"""Generate a markdown summary report from benchmark results.

Usage:
    python report.py --results-root ../../results/ --output report.md

Scans all benchmark result directories and produces a single markdown
report comparing baseline vs comprehend across all benchmarks.
"""

import argparse
import json
from pathlib import Path


def load_summary(results_dir: Path) -> dict | None:
    """Load summary.json from a results directory."""
    summary_path = results_dir / "summary.json"
    if summary_path.exists():
        with open(summary_path) as f:
            return json.load(f)
    return None


def generate_report(results_root: Path) -> str:
    """Generate markdown report from all available results."""
    lines = [
        "# Comprehend Benchmark Results",
        "",
        "Comparison of Claude Code with and without the comprehend skill.",
        "",
    ]

    benchmarks = [
        ("swebench_lite", "SWE-bench Lite"),
        ("swe_qa", "SWE-QA"),
        ("repo_qa", "RepoQA"),
    ]

    for bench_dir, bench_name in benchmarks:
        baseline_summary = load_summary(results_root / bench_dir / "baseline")
        comprehend_summary = load_summary(results_root / bench_dir / "comprehend")

        if not baseline_summary and not comprehend_summary:
            continue

        lines.append(f"## {bench_name}")
        lines.append("")
        lines.append("| Metric | Baseline | Comprehend | Delta |")
        lines.append("|:-------|:---------|:-----------|:------|")

        if baseline_summary and comprehend_summary:
            # Common metrics
            for key in baseline_summary:
                if key == "config":
                    continue
                b_val = baseline_summary.get(key, "—")
                c_val = comprehend_summary.get(key, "—")

                if isinstance(b_val, (int, float)) and isinstance(c_val, (int, float)):
                    delta = c_val - b_val
                    if isinstance(b_val, float):
                        lines.append(f"| {key} | {b_val:.3f} | {c_val:.3f} | {delta:+.3f} |")
                    else:
                        lines.append(f"| {key} | {b_val} | {c_val} | {delta:+d} |")
                else:
                    lines.append(f"| {key} | {b_val} | {c_val} | — |")
        elif baseline_summary:
            for key, val in baseline_summary.items():
                if key != "config":
                    lines.append(f"| {key} | {val} | — | — |")
        elif comprehend_summary:
            for key, val in comprehend_summary.items():
                if key != "config":
                    lines.append(f"| {key} | — | {val} | — |")

        lines.append("")

    if len(lines) <= 3:
        lines.append("No results found. Run benchmarks first.")
        lines.append("")

    lines.append("---")
    lines.append("Generated by `analysis/report.py`")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Generate benchmark report")
    parser.add_argument("--results-root", default="../../results",
                        help="Root directory containing benchmark results")
    parser.add_argument("--output", default="report.md",
                        help="Output markdown file path")
    args = parser.parse_args()

    results_root = Path(args.results_root)
    report = generate_report(results_root)

    output_path = Path(args.output)
    output_path.write_text(report)
    print(f"Report written to {output_path}")
    print(report)


if __name__ == "__main__":
    main()
